# ğŸ§  Self-Healing Classification DAG with Fine-Tuned Model


This project implements a robust sentiment classification CLI application using a fine-tuned Transformer model and a self-healing mechanism powered by LangGraph.

It performs inference using a primary model, evaluates confidence, and triggers fallback (via a user or a backup model) when necessary â€” prioritizing correctness over blind automation.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ cli.py                     # Main CLI interface
â”œâ”€â”€ model_utils.py            # Inference and backup model loading
â”œâ”€â”€ nodes.py                  # LangGraph nodes: ConfidenceCheck, Fallback
â”œâ”€â”€ fine_tuned_model/         # Pre-trained model directory (local or from HF)
â”œâ”€â”€ logs.txt                  # Logged predictions and fallbacks
â”œâ”€â”€ confidence_curve.png      # Auto-generated plot (optional)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # You're reading this!
```

---

## ğŸ”§ Features

- âœ… Fine-tuned DistilBERT for sentiment analysis (`tweet_eval` dataset)
- âœ… LangGraph DAG: Inference â†’ Confidence Check â†’ Fallback
- âœ… Confidence-based fallback with user clarification
- âœ… Backup zero-shot classifier using `facebook/bart-large-mnli`
- âœ… CLI logging (`logs.txt`)
- âœ… Confidence curve plot
- âœ… Fallback frequency summary

---

## ğŸš€ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/annie-2314/self-healing-classification-dag.git
cd langgraph-sentiment-cli
```

### 2. Create Virtual Environment (optional but recommended)

```bash
python -m venv env
source env/bin/activate     # On Windows: env\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Use Fine-Tuned Model

This project uses a locally fine-tuned `DistilBERT` model on the `tweet_eval` sentiment classification dataset.

If `fine_tuned_model/` is not present:

1. Run the fine-tuning script to train your own model:
   ```bash
   python train_model.py


---

## ğŸ§ª How to Run the Classifier

```bash
python cli.py
```

### Example

```bash
Input: I liked the visuals but the story was dull.
[InferenceNode] Predicted label: Neutral | Confidence: 52.13%
[ConfidenceCheckNode] Confidence too low. Triggering fallback...
[BackupModel] Trying zero-shot fallback model...
[BackupModel] Predicted label: Negative | Confidence: 82.43%
Final Label: Negative (From backup model)
```

---

## ğŸ“Š Bonus Features

### ğŸ“ˆ Confidence Curve

After several inputs, a plot is generated:

```
confidence_curve.png
```

It shows how confident the model was across inputs.

### ğŸ“‰ Fallback Frequency Summary

After exiting the CLI (`exit` command), you will see:

```
Fallback Stats:
- Total Inputs: 10
- Fallback Triggered: 4
- Backup Model Used: 3
- User Clarifications: 1
```

---

## ğŸ“ Deliverables Summary

| Deliverable              | Status       |
|--------------------------|--------------|
| âœ… Fine-tuned model       | Included / Download link |
| âœ… Source Code            | âœ”ï¸ All scripts provided |
| âœ… README.md              | âœ”ï¸ You're reading it |
| âœ… logs.txt               | Auto-generated |
| âœ… Demo video (optional)  | Add separately |

---

## ğŸ§  Model Notes

- Fine-tuned using HuggingFace `Trainer` on `tweet_eval` (sentiment)
- LoRA-based training is possible with PEFT (optional)
- Backup model uses `facebook/bart-large-mnli`

---

## ğŸ“¦ Requirements

```
transformers
torch
langgraph
matplotlib
huggingface_hub
```

---

## ğŸ“½ï¸ Demo Video

ğŸ“º *[Add YouTube or Drive link here]*

---

## ğŸ§‘â€ğŸ’» Author

**Your Name** â€“ `your.email@example.com`  
Project for: *LangGraph AI Challenge*

---
